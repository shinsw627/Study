# WIL 20211031

항해99 마지막 실전프로젝트 2주차에 접어들었다.

지금까지 배운 것을 바탕으로 실전프로젝트에는 어떤 기술스택을 사용할 것인가에 대한 고민을 많이 하였다. 

## 데이터베이스 선정 RDB vs NoSQL

- 데이터베이스
    
    데이터베이스는 엑셀을 떠올리면 된다. 데이터베이스의 가장 중요한 특성은 **구조화된 데이터를 관리**한다는 점이다. 'ID', '비밀번호'처럼 **컬럼에 따라 데이터를 정리**하고, 어떤 **기준을 통해 데이터를 정렬**하고, **필터링**할 수 있다. 굳이 엑셀 안 쓰고 데이터베이스를 쓰는 이유는, 데이터베이스라는 것 자체가 **데이터를 추가/변경/삭제/조회하는 작업이 더 구조화되어 있고 빠르기 때문**이다. 잘 모르겠다면 이고잉님의 강의인 [데이터베이스란?](https://opentutorials.org/course/195/1467)을 보고 오자.
    
    이 세상엔 수많은 형태와 종류의 데이터베이스가 있다. 형태로 치자면 `RDB(Relational Database)`와 `NoSQL(Not only SQL)`로 나눌 수 있고, 종류로 치자면 빅데이터 쿼리를 분산 처리하기 위해 개발된 PrestoDB, key-value 형태의 구조로 캐싱 용도로 자주 쓰이는 redis나 memcached, 시계열성 데이터를 저장하는 데에 특화되어 있는 InfluxDB같은 특별한 제품들이나 MySQL, PostgreSQL 등 전형적인 RDBMS가 있을 수 있다.
    
    위에서 이야기했듯 이번 의사결정은 서비스 운영을 위한 메인 데이터베이스를 결정하는 것이라, PrestoDB, druid, InfluxDB같이 **특별한 목적을 위해 만들어진 데이터베이스를 사용하진 않을 것**이다.
    
- 배경과 요구사항
    - **모든 스키마가 고정**되어 있다.
    - **엄청나게 빠른 속도를 요하진 않는다.**
    - 되도록이면 **AWS가 관리형으로 제공하는 인프라에서 사용**할 수 있어야 한다. EC2에서 **데이터베이스 서버를 직접 관리해야 하는 일이 없도록** 만들고 싶다.
- RDB를 선택 그 이유는,
    - **'NoSQL은 schemaless여서 더욱 유연한 형태로 데이터를 저장할 수 있다'**고 하지만, 우리가 개발하려는 것을 포함해 대부분의 서비스는 **스키마가 유동적인 경우가 거의 없다.**
    - NoSQL은 'scale out(접속된 서버의 대수를 늘려 처리 능력을 향상시키는 것)이 가능해서 **scale up만 가능한 RDB에 비해 단일 장애 지점도 없고 비용도 적고 클러스터에서 잘 동작**하도록 만들어져 있다'라며 퍼포먼스와 장애 대응에 관한 메리트를 어필하곤 하는데, RDB에서도 **master-slave 모델**을 사용하면 이런 문제들을 충분히 커버할 수 있다. 이는 **읽기 전용 인스턴스(slave)를 만들어 데이터를 복제**시키는 것인데, 이정도만 해도 웬만한 수준의 서비스 트래픽을 다 감당할 수 있다고 판단했다. master가 죽은 경우 slave가 master로 승격하는 구조를 만들면 장애 복구도 잘 되고 말이다.
    - 평균적으로 NoSQL이 RDB보다 빠른 것이 맞고, 동일한 비용을 사용했을 때 퍼포먼스로 치면 웬만하면 NoSQL이 가성비가 좋다. 그러나 이건 우리가 **어떤 RDB 제품을 사용하느냐에 따라** 다르다.
    - NoSQL 데이터베이스 서버를 AWS에서 운영하려면, 대부분의 경우 **EC2 위에서 직접 돌려줘야** 한다. MongoDB의 경우 최근에 출시된 [DocumentDB](https://aws.amazon.com/ko/blogs/aws/new-amazon-documentdb-with-mongodb-compatibility-fast-scalable-and-highly-available/)를 사용해볼 수 있는데, 아시아 쪽에선 아직 서비스가 오픈되지 않았다. 결국은 **관리 포인트가 늘어날 것이며 이를 감당하기 어려운 상태**다.
- 데이터베이스 호스팅 위치
    
    ### 선택지
    
    데이터베이스 서버 관리를 직접 하느냐, AWS에게 관리를 양도하느냐의 차이다.
    
    - EC2
    - RDS(Relational Database Service)
    
    RDS를 선택 그 이유는,
    
    - **동일한 워크로드(하드웨어 요구량)**를 기준으로 했을 때 EC2의 비용이 더 적을 수는 있겠으나, **데이터베이스 서버를 직접 운영**하는 것은 정말 **큰 관리 포인트**다. 백업과 복구 정도만 생각해도 아득하다. 이런 건 그냥 돈 좀 더 내는 게 낫다고 생각한다(물론 여기선 어차피 free tier를 사용)
    - 관리형 서비스가 없는 데이터베이스를 선택한 경우 어쩔수 없이 EC2에 올려야겠지만, RDS는 **대부분의 메이저한 RDB 엔진**을 지원하며 그 외의 RDB 중에서는 **굳이 EC2에서 직접 프로비저닝할 정도의 메리트가 있는 것도 딱히 없다.**

## 개발 프로세스 정립

- 개발 프로세스란?
    
    개발 프로세스는 어떻게 **이슈를 관리**하고, 어떤 방식으로 **작업을 진행**하고, 완료된 작업은 **어떤 과정을 거쳐서 실제 제품에 반영시킬지**와 같은 것들을 규칙화시킨 것이다. 개발 프로세스의 정립이 필요한 이유는 다음과 같다.
    
    - 프로젝트 관리자 입장에서 각 개발자에게 할당된 **작업들이 어떤 상태인지(준비 중/진행 중/완료)**를 쉽게 알 수 있다.
    - 이슈를 실제로 처리하는 입장에서, 개발 프로세스에 따라 작업을 진행하면 어느 브랜치에서 어떤 이름으로 브랜치를 생성할지/어느 브랜치로 pull request를 올리며 리뷰를 받아야 할지/master merge 후 어떤 후속 작업을 진행해야 하는지와 같은 고민을 줄일 수 있다. **실제로 작성하는 코드에만 집중하는 환경**을 만들 수 있다.
    - 잘 정립된 개발 프로세스는 **생산성을 높이는 것에 기여**하며, **작업의 진행을 매끄럽게** 만들고, **결과물의 퀄리티**를 높인다.
- 배경과 요구 사항
    - 이슈 각각에 대해 **작업자를 assign**할 수 있다.
    - **이슈에 대해 커뮤니케이션**할 수 있어야 한다.
    - 태그/라벨 등으로 **이슈의 종류를 구분**할 수 있어야 한다.(feature, hotfix, enhancement 등)
    - **이슈의 상태(To do, In progress 등)를 구분**해서 **시각화**할 수 있어야 한다.
    - 되도록 GitHub 내에서 해결할 수 있으면 더 좋다.
- GitHub Issues & Projects를 선택 이유는?
    - Jira는 무조건 돈이 들어간다. 그렇다고 큰 메리트가 따로 있다고 생각이 들진 않는다. **큰 조직에서 매니저 급이 태스크를 관리**할 때나 잘 쓰일 것 같은 트래커라고 생각한다.
    - Trello는 모던한 형태의 태스크 관리 보드인데, 필요한 기능들이 잘 들어가 있고 괜찮은 툴이지만 **지금의 요구사항으론 GitHub 외부의 트래커를 따로 쓸만한 이유가 없다.**
    - 위의 요구 사항을 모두 만족한다 - **assign**, **issue conversation** 기능이 있고, **customized label**과 **filter by label** 기능을 통해 이슈 종류 구분이 원활하고, **projects**에 **issues** 기능을 연동해 이슈를 시각화 가능하며, projects의 **automation** 기능을 통해 **이슈 구분**과 **이슈 상태 자동 갱신**도 가능하다.(이슈가 close되면, projects에서 'Done' 컬럼으로 자동 이동시키는 등)
    - GitHub Issues에서 이슈를 등록하면 해당 이슈에 대한 번호가 매겨지는데, 커밋 메시지에 `#123`과 같이 그 번호를 명시해 두면 **자동으로 해당 이슈가 링크**되며 conversation할 때도 사용할 수 있다.(참고로 이건 Jira와 BitBucket을 연동하는 경우에도 가능하다.)
    - **이슈 트래커를 위해 따로 돈을 내지 않아도 된다.** private repository를 생성 가능하게 만들기 위해 Organization 유료 플랜을 결제하는 경우는 있을 수 있지만, GitHub issues와 projects는 항상 무료다. 아래의 캡처들은 순서대로 각각 **Projects**와 **Issues** 기능이다.
- **개발 진행 과정 어떻게 적용할 것인지(우리가 사용할 방식)**
    1. 개발자의 공수가 필요한 작업이 생기면, **알맞는 라벨과 함께 이슈를 생성**하고, 프로젝트 보드와 동기화시키기 위해 **Projects를 선택**해 둔다. 이슈가 **'To Do'**에 위치할 것이다.
    2. 해당 작업을 진행할 개발자가 정해지면, **assign**해 둔다.
    3. 개발자는 `issue/<이슈 번호>` 네이밍을 가진 브랜치를 dev에서 체크아웃한다. 이슈 번호가 9번이라면, `issue/9`같은 식이다.
    4. 작업이 완료되면 **동료 개발자에게 리뷰를 요청**하고, 확인후 dev에 merge 한다.
    5. 문제 없이 작업이 완료되었다면 **issue를 close**하고 **작업 브랜치를 제거**한다.

## https적용

- http는 보안이 취약하므로 https 를 적용하기로 결정 했다.
    
    사전 준비  : 배포 되어 실행 된상태 , 도메인 등록(가비아)
    
    letsencrypt 인증서를 EC2에서 설치 한 후에 인증서를 발급받은 후 그 인증서를 다시 letsencrypt가 접근하게 만들어 https적용 시킴
    

## 로그인 구현시에 어떤 방법을 사용할까?

- 로그인 구현방식 결정
    
    프론트랑 협업하는 과정에서 당장 둘다 https 전환이 힘들어서 테스트가 용의 하지 않았다. 따라서 기존에 기획했던 session방식의 로그인 구현 말고 기존에 썼던 JWT 헤더 인증 방식을 택했다.
    
- jwt를 어디에 보관할 것인가
    
    처음 생각한 방법은 localstorage나 cookie를 이용하는 것이었다.
    
    하지만 localstorage, cookie 모두 장단점이 존재한다.
    
    xss가 뚫리면 localstorage, cookie 정보 모두 탈취가 가능하다.
    
    ✔ xss란?
    
    공격자가 의도하는 악의적인 js 코드를 피해자 웹 브라우저에서 실행시키는 것.
    
    ✔ CSRF란?
    
    정상적인 request를 가로채 피해자인 척하고 백엔드 서버에 변조된 request를 보내
    
    악의적인 동작을 수행하는 공격을 의미한다.
    
    **1. LocalStorage**
    
    쿠키와 달리 자동으로 request에 담기진 않으므로 csrf 공격에는 안전하다.
    
    xss가 뚫리면 글로벌 변수인 localstorage 정보가 탈취 가능해진다.
    
    **2. Cookie**
    
    httpOnly 옵션을 이용해서 js에서 쿠키 자체에 접근하지 못하도록 하면
    
    LocalStorage에 비해 xss 공격으로부터 비교적 안전하다.
    
    자동으로 request에 담겨 보내지기 때문에
    
    공격자가 사용자가 관련 이미지 링크를 클릭하도록 유도하여 request를 위조하기 쉽다.
    
    **3. accessToken, refreshToken**
    
    accessToken은 서버에 요청 시 인증을 위한 토큰이다.
    
    짧은 생명주기를 갖고 js private variable에 저장한다.
    
    js private variable은 새로 고침하면 사라지는 휘발성이기 때문에
    
    페이지 이동 시와 accessToken 만료 시 accessToken을 재발급할 수 있도록
    
    accessToken에 비해 비교적 긴 생명주기를 갖는 refreshToken을 사용하고
    
    refreshToken은 httpOnly로 쿠키에 저장한다.
    
    만일 refreshToken이 csrf에 의해 사용된다 하더라도
    
    csrf는 서버 동작을 일으키는 공격 방법이기 때문에
    
    공격자는 응답으로 오는 accessToken을 알 수 없다.
    
    httpOnly 쿠키를 사용하여 xss를 통한 탈취를 막고
    
    refreshToken 방식을 이용해 csrf를 막는다.
    
    Access Token(JWT)를 통한 인증 방식의 문제는 만일 제 3자에게 탈취당할 경우 보안에 취약하다는 점입니다.
    
    유효기간이 짧은 Token의 경우 그만큼 사용자는 로그인을 자주 해서 새롭게 Token을 발급받아야 하므로 불편합니다. 그러나 유효기간을 늘리자면, 토큰을 탈취당했을 때 보안에 더 취약해지게 됩니다.
    
    이때 “그러면 유효기간을 짧게 하면서  좋은 방법이 있지는 않을까?”라는 질문의 답이 바로 **"Refresh Token"**입니다.
    
    access token은 글자 그대로 사용자가 서버에 request를 보낼 때 header에 함께 보내는 접근용 토큰으로 만료 기간을 10~15분으로 짧게 두어 자주 발급받게 한다. 이로써, 만약 access token이 다른 사람에게 도난당하더라도 금방 만료되어 안정성이 유지된다.
    
    refresh token은 access token이 만료됐을 때, access token을 재발급받기 위한 인증용 토큰이다. 평소의 요청에는 request에 포함되지 않으면 오직 access token을 재발급받는 용도로만 사용한다. 만료 기간은 자동 로그인 기능을 몇 달로 지정할 것인 가에 따라 조절하면 되고, 만료 기간이 끝나면 사용자에게 로그인을 다시 요청하면 된다.
    
    소셜 로그인 Resource Server의 Access Token, Refresh Token, Client의 Access Token, Refresh Token 양쪽다 사용하는 것이 의미가 있는가???????
    
- 소셜 로그인 시 Passport를 사용하는 이유
    
    패스포트란? Node용 인증 미들웨어!요청 인증이라는 단일 목적을 제공하기 위해 설계
    
    우리가 구현하고자 하는 로그인 종류는 총 3가지 
    
    1. 로컬 로그인
    2. 카카오로그인
    3. 구글 로그인
    
    따라서 각각의 인증 방법, 구현 방법이 다 다르기 때문에 작업이 굉장히 복잡해진다. 
    
    **그렇다면 passport를 사용하면 어떤 장점이 있길래 직접 구현하지 않고 passport를 사용하는가?**
    
    1. 세션과 쿠키 처리 등의 복잡한 작업이 많기 때문에 검증된 모듈을 사용하는 장점.
    2. 카카오, 구글, 각각의 인증기관의 각각 다른 인증 방법, 구현 방법의 복잡성을 어느정도 통일화 시킬 수 있는 장점.
- Passport이용한 인증 과정
    
    ### 로그인 전
    
    1. 로그인 요청
    2. passport.authenticate 메서드 호출
    3. 로그인 전략 수행
    4. 로그인 성공 시 사용자 정보 객체와 함께 req.login 호출
    5. req.login 메서드가 passport.serializeUser 호출
    6. req.session에 사용자 아이디만 저장
    7. 로그인 완료
    
    ### 로그인 후
    
    1. 모든 요청에 passport.session() 미들웨어가 passport.deserializeUser 메서드 호출
    2. req.session에 저장된 아이디로 DB에서 사용자 조회
    3. 조회된 사용자 정보를 req.user에 저장
    4. 라우터에서 req.user 객체 사용 가능
- 소셜 로그인 처리 방식변경(10.28)
    
    소셜 로그인 처리 과정에서 처음 계획한 것은 passpor모듈을 이용해 처리하는 것이었다. 
    
    즉  프론트에서 api 호출 을 했을 때 백에서 passport로 소셜 api 를 통해 로그인 정보(회원가입정보) 를 가져와서 db에 저장한후 토큰을 만들어 프론트에 보내주는 방식이었다. 그런데 막상 프론트와 연결을 했을 때 local로그인은 잘 되었지만 소셜 로그인은 되지 않은 불상사가 일어나고 말았다. 계속 cors 에러가 뜨면서 접근이 거부 되고 하루종일 매달려봤지만 해결하지 못하였다. ( 프록시서버도 생각 해봤음) 그래서 결국 프론트에서 소셜api를 통해 토큰을 가져와서 우리가 그 토큰으로 다시 소셜에 요청해서 유저 정보를 받아오는 방식으로 바꾸었다.   
    

## 왜 Bearer 인증 방식인가?

- 사용자 인증방식 도입이유
    
    HTTP는 **연결 지향 프로토콜인 TCP 기반**임에도 불구하고, 대표적인 **비연결 지향 프로토콜**이다. 따라서 **한 번의 요청-응답 사이클이 완료되면 연결을 종료**하기 때문에, 동일한 클라이언트가 요청을 아무리 많이 하더라도 프로토콜은 이를 **모두 독립적인 요청으로 인지**한다. 이 때문에 클라이언트는 **매 HTTP 요청마다** 본인이 누구인지를 인지시킬 수 있는 **인증 정보(credential)를 요청의 어딘가에 포함**시켜야 하며, 서버 또한 클라이언트의 자원 접근을 허용하기 전에 이러한 인증 정보를 기반으로 인증 과정을 일차적으로 거쳐야 한다. **사용자 A가 작성한 게시글**을, **다른 사용자가 마음대로 수정/삭제할 수 없게** 만들어야 하기 때문이다.
    
- 인증정보위치
    
    **모든 형태의 HTTP 요청**에 다 사용 가능해야 한다. 예를 들어, **GET 요청에서 사용할 수 없으면 안된다.**
    
    HTTP 표준에 맞춰지면 더 좋다.
    
    클라이언트 사이드에서, **쉽게 저장**하고 HTTP **요청 단에서 쉽게 데이터를 실어줄 수 있어야** 한다.
    
    선택지 ( request body,요청의 query parameter,Cookie 헤더,Authorization 헤더)
    
- **Authorizaton 헤더**를 선택 그이유는?
    
    인증 데이터는 **메타데이터** 성격이 강하다. request body와 어울리지 않는다.
    
    **request body를 사용할 수 없는 메소드**가 있다. GET, HEAD, DELETE, TRACE가 그렇다.
    
    url의 `?` 뒤에 붙는 query parameter는 고려해볼 만 하지만, 사용자 인증 하라고 Authorization 헤더가 표준화되어 있는데 **굳이 query string을 써서 얻을 메리트가 없다.**
    
    Cookie는 헤더를 사용한다는 점에서 Authorization과 비교해볼 만 하다. 하지만 이것도 위에서 query parameter를 걸렀던 이유와 비슷하게, **인증이라는 맥락은 Authorization이 더 어울린다.**
    
- 인증스키마
    
    이제 사용자가 로그인을 했을 때, 서버는 **그 사용자를 나타내는 특별한 값**을 만들어서 전달해 **권한을 부여**하고, 사용자는 나중에 **Authorization 헤더로 그 인증 데이터를 보내준다**는 것까지 결정이 되었다. '사용자를 나타내는 값'을 어떻게 만들어낼 지는 표준이 결정해 줄 것이다. Authorization 헤더에는 **값에 대한 표준**도 있으니까.
    
    Authorization 헤더의 value는 `<type> <credentials>`처럼 생겨먹도록 하는 것이 표준이다. `Bearer xmp98-cb35.potn6jz.zorj15gmb-`이 한가지 예다. **인증 타입에 따라 credential을 만들어내는 방식이 정해져** 있기 때문에 맘대로 할 수 있는 부분이 아니다. 표준을 따르지 않더라도 이유는 있어야 한다. 그러니 **인증 스키마에 대한 의사결정**을 진행하자.
    
    표준 상 Authorization 헤더의 값에는 **RFC에 의해 표준화된 인증 스키마**를 사용할 수 있게 되어 있다.
    
    - Basic
    - [비표준] OAuth 1.0a를 사용하는 Bearer
    - OAuth 2.0을 사용하는 Bearer
    - [비표준] JWT, 또는 JWT를 사용하는 Bearer
    - Digest
    - HOBA
- **JWT을 사용하는 Bearer**를 선택하겠다. 그 이유는?
    - Basic은 **ID와 비밀번호를 base64 인코딩**하는 방식이다. base64는 **별도의 key 없이도 복호화가 가능**한 인코딩이므로, 안전하지 않다.
    - **OAuth 1.0a는 Bearer 인증 표준이 아니다.** Bearer 스펙을 명시한 [RFC 6750](https://tools.ietf.org/html/rfc6750)에는 큰 글씨로 **'The OAuth 2.0 Authorization Framework'**라고 되어 있기까지 하다.
    - Bearer에서 사용하는 OAuth 2.0 방식의 인증은 확장성이 매우 높다. **'Facebook 계정으로 로그인'**과 같은 기능이 OAuth로 구현되었다. 되도록 이런 흐름에 낄 수 있다면 좋겠지만, OAuth 2.0은 자체 암호화를 지원하지 않기 때문에 **HTTPS**를 쓰는 것을 권고하고 있고, 돈이 들어가야 하는 부분이다. 인증 정책은 나중에 **HTTPS 관련 비용 문제를 해결하고 나서 변경해도 괜찮을 것 같다**는 판단이다. 또한, 스펙 자체에서 **명확하게 정의하지 않은 부분**이 꽤 있어서 그만큼 고민이 깊어진다고 한다.
    - **Bearer에 JWT를 사용**하거나, **JWT라는 타입을 쓰는 것도 표준이 아니다.** 그러나 HTTPS 문제로 OAuth 2.0을 보류하게 되니, **대신 쓸 토큰 기반 인증 시스템**으로 JWT가 가장 쓸만 하다.
    - **'보호된 리소스에 대한 접근 권한을 부여받기 위해 제시하는 유일한 작업이 토큰을 전달하는 것 뿐'**일 때, 이 토큰을 **bearer token**이라고 부를 수 있다. 따라서 JWT를 사용하는 인증 방식도 사실상 bearer라는 문맥에서 벗어나지 않으며, 단지 **bearer token을 생성하기 위해 OAuth 2.0 관련 사양을 사용하지 않는 것 뿐**이다. **Authorization 헤더**를 사용하고, **디지털 방식으로 서명(sign)**된 토큰을 사용한다면, 이정도 사이즈의 프로젝트에서는 비용을 들이면서까지 OAuth 2.0을 완전히 수행하려 하지 않아도 된다고 생각한다.
    - JWT는 사용 사례가 많고, 거인의 어깨(잘 만들어진 라이브러리, 예제 등)가 잘 준비되어 있다.